---
sidebar: sidebar
permalink: healthcare/ehr-epic-performance_test_methodology.html
keywords: test, methodology, generationio, tool, genio, storage, harware, software, vms
summary: The GenerationIO tool (GenIO) is used by Epic to validate that storage is production ready. This test focuses on performance by pushing storage to its limits and determining the headroom on storage controllers by ramping up until requirements fail.
---

= Test methodology
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./../media/

//
// This file was created with NDAC Version 2.0 (August 17, 2020)
//
// 2021-05-20 13:41:30.097635
//

== Test plan

The GenerationIO tool (GenIO) is used by Epic to validate that storage is production ready. This test focuses on performance by pushing storage to its limits and determining the headroom on storage controllers by ramping up until requirements fail.

The tests performed here are focused on determining headroom as well as using Adaptive Quality of Service (AQOS) to protect critical Epic workloads. For AFF A300 testing, two servers are used with GenIO loaded on both to drive I/O on the storage controllers. Three servers are used with GenIO loaded on all three to drive I/O on the AFF A700 storage controllers. Three servers are used because of server performance limits, and three servers are required for an AFF A700.

== Test environment

=== Hardware and software

For this study, we configured three Red Hat Linux virtual machines (VMs) on VMware ESXi 6.5 running on Cisco UCS B200-M5s. We connected the ESXi hosts to the AFF storage controller nodes with Cisco MDS-series switches by using 16Gb FC on the server side and 16Gb FC on the storage side. The AFF A700 nodes were connected to one DS2446 disk shelf with 3.8TB SSDs by following NetApp cabling best practices.

The three tables below list the hardware and software components that we used for the Epic performance test configuration.

The following table lists Epic Test hardware and software components.

|===
|Hardware and software components |Details

|Operating system for VM
|RHEL 7.4 VMs
|Operating system on server blades
|VMware ESXi 6.5
|Physical server
|Cisco UCS B200 M5 x 3
|Processors per server
|Two 20-core Intel Xeon Gold 6148 2.4Ghz
|Physical memory per server
|768GB
|FC network
|16Gb FC with multipathing
|FC HBA
|FC vHBA on Cisco UCS VIC 1340
|Dedicated public 1GbE ports for cluster management
|Two Intel 1350GbE ports
|16Gb FC switch
|Cisco MDS 9148s
|40GbE switch
|Cisco Nexus 9332 switch
|===

The following table lists NetApp AFF A700 and AFF A300 storage system hardware and software.

|===
|Hardware and software components |AFF A700 details |AFF A300 details

|Storage system
|AFF A700 controller configured as a high-availability (HA) active-active pair
|AFF A300 controller configured as a high-availability (HA) active-active pair
|ONTAP version
|9.4
|9.5
|Total number of drives
|36
|24
|Drive size
|3.8TB
|3.8TB
|Drive type
|SSD
|SSD
|FC target ports
|Eight 16Gb ports (four per node)
|Eight 16Gb ports (four per node)
|Ethernet ports
|Four 10Gb ports (two per node)
|Four 10Gb ports (two per node)
|Storage virtual machines (SVMs)
|One SVM across both node aggregates
|One SVM across both node aggregates
|Ethernet logical interfaces (LIFs)
|Four 1Gb management LIFs (two per node connected to separate private VLANs)
|Four 1Gb management LIFs (two per node connected to separate private VLANs)
|FC LIFs
|Four 16Gb data LIFs
|Four 16Gb data LIFs
|===

The following table lists NetApp AFF A700 and AFF A300 storage system layout.

|===
|Storage layout |AFF A700 details |AFF A300 details

|SVM
|Single SVM for Epic application databases
|Single SVM for Epic application databases
|Aggregates
|Two 20TB each
|Two 30TB each
|Volumes for production
|Sixteen 342GB volumes per RHEL VM
|Sixteen 512GB volumes per RHEL VM
|LUNs for production
|Sixteen 307GB LUNs, one per volume
|Sixteen 460GB LUNs, one per volume
|Volumes for journal
|Two 95Gb volumes per RHEL VM
|Two 240Gb volumes per RHEL VM
|LUNs for journal
|Two 75Gb LUNs, one per volume
|Two 190Gb LUNs, one per volume
|===
